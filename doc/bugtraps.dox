/**
   \page page4 Bug traps


The first step to avoid bugs is to understand their origin,
but even if you know where they come from, you are still
inadvertedly going to produce bugs. 
The second step is to trap and fix
these bugs as soon as they are written. Bugs could be created
when you first write code, or when functionality is extended or
simply as a result of
refactoring code in order to improve the design or performance.

Whenever a new bug is found, write a test that catches it, and then fix the bug.
By keeping the test in your test suite, you don't <span class="quote">allow the same bug to bite you twice</span> \cite Solid:maguire.

\section CodeReview Code review

<blockquote>
Why do you see the speck that is in your brother’s eye, but do not notice the log that is in your own eye?
<div id="author">
Matthew 7:3
</div>
</blockquote>

It is often amazing how easy it is to spot other people's bugs,
when you need hours to find your own. This phenomenon
has simple psychological explanations. One the one hand,
most of us have a low degree of self-criticism.
This is a good thing, because self-criticism is often
associated with depression. On the other hand, when reviewing our own code,
we know what it is supposed to do, and the eyes see what you expect.
Hence, the typos and errors escape you. Not so when reading other
people's code; you have to read carefully to understand how the code works,
and you spot the bugs as easy as
cockroaches in plain daylight.

Peer code review could be carried out in many ways. From the simple read over-the-shoulder variant, via e-mail reviews,
to a formal peer review meeting with the presence of quality assurance officers and writing of review documents.
The over-the-shoulder variant is often very fruitful when you are stuck at some problem. Your colleague will often
pinpoint the error and it is often trivial to fix. The pair programming technique makes full use of this effect.
Two programmers work together at a workstation. The driver writes the code while the observer reviews each line
as it is typed.
Roles are switched frequently. However, for pushing your code to the main developing
branch of your central repository, a formal tool-based peer review is recommended. Several tools for software peer review
are available on the market. They will integrate with your software version control system and make the technical aspects
of peer review simple. They also document the review process automatically.




\section CompileTimeTests Compile-time tests

<blockquote>
To err is human, to persist in error is diabolical.
<div id="author">
Georges Canguilhem
</div>
</blockquote>

What is better than to catch errors early, even before they are compiled.
All assumptions made when designing and implementing the code which is testable at compile time, should be tested during compilation.
C++11 and later has the declaration `static_assert'.
\code{.cpp}
  static_assert(constant-expression,string-literal)
\endcode
This is a compile-time check. If the constant-expression evaluates to false,
the compilation fails with the string-literal as an error message.
The `static_assert' declaration
is especially useful in combination with the classes
from the type support library classes of the `<type_traits>'
header.
It then provides safeguards against bugs created by wrongful use of templates.
It also helps to reveal critical differences when porting software to new platforms,
provided all assumptions have been expressed in static assert declarations.

A very simple example of its use
is shown in the  <a href="byte__swap_8h_source.html">byte swap code</a>.


\section RunTimeTests Run-time tests

<blockquote>
Fundamentally you can’t do your own QA, it’s a question of seeing you own blind spots.
<div id="author">
Ron Avitzur
</div>
</blockquote>

In the design by contract
methodology, the design is expressed as assertions
embedded in the source. For example, restrictions on input parameters
(preconditions) are checked run-time. If a precondition is violated,
an exception   is thrown. For a simple example
on the  <a href="Circle_8cpp_source.html">radius of a circle</a>. 
These kind of run-time tests guards against wrong use of classes and functions.

Similarly, one could put in postconditions on floating point
computations to ensure that
the result never is a NaN  (Not a Number).

Of course, spreading extra run-time checks everywhere in your code
will slow it down. If speed is important in your application, you
could keep the tests on during development and testing phase
and disable
them with a preprocessor directive in the final release shipped to customers.
In the examples of
Dbc macros  we have used the
`NDEBUG' macro to turn off
checking. If `NDEBUG' is set, the `PRECONDITION', `POSTCONDITION' and  `INVARIANT' macros all turn void.
In real code you may want to have different testing levels depending on the
state of your code, and possibly catch the exceptions and log the user input and
the errors to  a log file for easier debugging.


\section MemoryErrorTools Memory error detection tools

Another aspect of run-time testing is to run a memory error detector
such as `valgrind'. Such tools detect use of
uninitialized memory, errounous memory access, and memory leaks. Below is a transcript of a valgrind run of the example unit tests:
\include valgrind.tex
In this case there were no memory leaks and no errors. If your code produces strange errors, perhaps errors that are
irreproducible, then always check the code with a memory error detector before debugging. It could save you a lot of time.
For the same reason, all baselines of your main branch should be checked for memory errors.


\section Continuous Continuous integration and automated testing

Continuous integration (CI) \cite Booch:CI
is a software development practice where
members of a team regularly integrate their work into a common mainline.
Each such integration is followed by automated builds and tests.
This ensures that new defects are detected as quickly as possible.
In addition, by integrating often,
there are fewer merging problems since each developer's
own branch never deviates far from the main branch.

The key point---from a bug trapping point of view---is that the test
procedures should be triggered <em>automatically</em>.
This ensures that the tests are done and bugs are fixed  as early as possible.
 Jenkins \cite Jenkins is a popular tool for automatization of tests. 


\section UnitTesting Unit testing
<blockquote>
Beware of the above code; I have only proved it correct, not tried it.
<div id="author">
Donald Knuth
</div>
</blockquote>


A software unit could be a component, a module, a class, or a method. In unit testing 
a unit refers to the smallest testable part of an application.
In C++ a unit is a usually
a class or invidual methods. Ideally, each test  should be
independent from the others, but in reality
 methods of a class often depend on other methods from the same or from other classes.
Hence, any dependencies to
other units should be eliminated by the use of fakes, stubs or mocks (a dear child has many names).
Otherwise there will be dependencies to other units and the tests
will be less reliable, and more costly to maintain.
In order to be able to control the dependencies of a unit, the code has to be designed
so that one is able to inject mocks for all dependencies when setting up the test.
In the code example `class NeedsEngine' we give an example of a pattern \cite BeautifulInjection, for
injecting the engine dependency in a car class.



Each method should be tested 
in a separate test so that the failure of a test
easily is traced back to the offending unit.
Unit tests are often collected in test suites,
where a suite often correspond to a whole class or a module.

The unit tests determine whether a given feature works as intended by the programmer.
For each class and function, the programmers write as many automated tests
as they can think of that might catch <span class="quote">broken code</span>.
Only when all tests, new and old,
run successfully, the code is integrated into the development tree.
In this way, a rich 
collection of test suites guard against introduction of new errors,
or <em>regressions</em>, 
in existing functionality after adding new features
or after refactoring. This comes with a bit of a caveat, however. Having many unit tests,
increases the cost of
changing code. 

It is very important to document the tests and the expected return values
(the <span class="quote">truth</span>) for all functions so that the tests can be reviewed
and understood by others. It follows that the test code
should follow the same stringent coding standard as the production code itself.

The test and its explanation document the <em>specification</em>
of each tested unit. In test-driven development (TDD) 
 requirements 
are translated into test cases and the software is
developed so that it passes the new tests \cite TDD:Wikipedia.
TDD is characterized by a short development cycle. This development process
ensures that the code is designed to be testable.

It is, however, important to realize that unit testing is <em>not</em>
an effective way of detecting bugs. More often than not, 
bugs arise in the interaction between software units.
Even if components A and B both work independently,
they may not be compatible with one another or configured correctly.
Such errors are caught during <em>integration testing</em>.





\subsection GoogleTest The Google Test framework

Implementing tests for all classes and methods
may appear to be a daunting task, and indeed 
it is a big task. As always in programming, the trick is to divide and conquer. 
If you write tests for each function 
you write, then you have a large test suite before you know it.
The job of keeping track of the tests,
and making sure that they are run, is a simple routine task which should be automated.

Several frameworks 
exist for unit testing of C++ code. Do an internet search for <span class="quote">Unit testing C++</span> and you
should get a lot of hits.
In this book the concept of unit testing is illustrated with
GoogleTest, also known as gtest, \cite GoogleTest. This
framework was chosen because it has:
<ol>
<li> Minimal coding overhead when writing tests.</li>
<li> Automatic test registration.</li>
<li> Support for suites or grouping of tests.</li>
<li> Support for fixtures.</li>
<li> Good scalability.</li>
<li> Possibility to run subsets of the tests using  wildcards.</li>
</ol>
For examples of unit tests written with GoogleTest,
see \link UnitTests "Unit Tests"\endlink



\subsection GoodUnitTests Writing good unit tests


To be able to write good unit tests, the code should be testable
(see the chapter \ref page5). By testable I mean that the unit
is designed so that
tests can be written with a reasonable amount of effort. Moreover,
changes to other units should have minimal impact on the tests.

A good unit test suite should cover all relevant input, forcing
the code to visit all possible branches during the tests.
Use tools for checking
test coverage (see <a href="bugtrap/code/index.html">Test coverage</a>).
Parameter space should be tested at the limits, beyond the legal limits, and at special values in between.

Keeping the classes small and focused helps to make the task of writing unit tests managable.
Remember that as you increase the number of parameters to a function, there is a combinatorical explosion
of combinations to test. Consider a n-parameter function \f$f(x_0,\ldots, x_{n-1})\f$
with valid parameters \f$x_i \in [x_{\text{min}(i)},x_{\text{max}(i)}]\f$.
Testing the limits, beyond limits and in the middle, gives five
test values per parameter.
There are then \f$5^n\f$ combinations to test for this function. Already at three parameters, the situation is daunting.

Run-time errors such as illegal input to functions
should be caught by assertions or exceptions.
For very well tested code, where performance is of
the essence, and
where run-time errors are guaranteed never to happen,
the assertions can be turned off in release builds.




\section Integration Integration  testing

<blockquote>
The bitterness of poor quality remains long after the sweetness of meeting the schedule has been forgotten.
<div id="author">
Anonymous
</div>
</blockquote>

Once all the individual units have been created and unit tested,
they are integrated into larger modules. The collective behaviour of
this integrated whole needs to be tested, too. The main goal
of integration testing is to test the
interaction and cooperation between the units.
Does the whole work as intended in all situations?
Many bugs
are created in the integration phase, because the developer assembling the
parts does not know the correct use of the parts. This happens
if the interfaces are poorly documented, or the documentation
is not read because it is hard to find or there is a
time pressure because of a rapidly approaching delivery date.



\section System System  testing

<blockquote>
The problem is not that testing is the bottleneck.
The problem is that you don’t know what’s in the bottle.
That’s a problem that testing addresses.
<div id="author">
Michael Bolton
</div>
</blockquote>

System testing is done on the whole system. It could either be done after
all the other parts have been integrated and assembled into a complete system (bottom-up testing),
or at an early stage where subsystems have been
replaced with stubs or mockups (top-down testing). More often than not,
a combination of these two scheemes is used. Postponing system testing
until everything else is finished is very risky. If something is amiss,
a lot of interfaces may have to change. On the other hand,
a full verification of all the system requirements is not possible until
all the code is in place. Hence, a combined approach is called for.

Ideally there should be one test for each system requirement.
A test description for each of these tests should be available when implementation of its realization starts.
Then one can implement a test before the requirement is met by the system.

Normally there are performance requirements on the system.
Hence, there should also
be performance tests in the test suite.
Obviously the performance tests should be built with the relevant compilator optimization flags turned on.

The collection of system tests should be maintained and refined during the development stages,
and after each code commit, there should be a system for automatically running all
the unit tests and system tests automatically both in debug mode and in release mode (see e.g. \cite Jenkins).
The tests should be built and run on all
the relevant computer platforms.

\section BugTrapConclusion Concluding remarks on bug traps


*/

