<!-- HTML header for doxygen 1.8.14-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="cache-control" content="max-age=86400"/>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.15"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Bugtrap: Bug traps</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js", "TeX/mhchem.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" async="async" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="bugtrap.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Bugtrap
   </div>
   <div id="projectbrief">How to code with fewer bugs</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.15 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Bug traps </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The first step to avoid bugs is to understand their origin, but even if you know where they come from, you are still inadvertedly going to produce bugs. The second step is to trap and fix these bugs as soon as they are written. Bugs could be created when you first write code, or when functionality is extended or simply as a result of refactoring code in order to improve the design or performance.</p>
<p>Whenever a new bug is found, write a test that catches it, and then fix the bug. By keeping the test in your test suite, you don't <span class="quote">allow the same bug to bite you twice</span> <a class="el" href="citelist.html#CITEREF_Solid:maguire">[15]</a>.</p>
<h1><a class="anchor" id="CodeReview"></a>
Code review</h1>
<blockquote class="doxtable">
<p>Why do you see the speck that is in your brother’s eye, but do not notice the log that is in your own eye? </p><div id="author"> Matthew 7:3 </div> </blockquote>
<p>It is often amazing how easy it is to spot other people's bugs, when you need hours to find your own. This phenomenon has simple psychological explanations. One the one hand, most of us have a low degree of self-criticism. This is a good thing, because self-criticism is often associated with depression. On the other hand, when reviewing our own code, we know what it is supposed to do, and the eyes see what you expect. Hence, the typos and errors escape you. Not so when reading other people's code; you have to read carefully to understand how the code works, and you spot the bugs as easy as cockroaches in plain daylight.</p>
<p>Peer code review could be carried out in many ways. From the simple read over-the-shoulder variant, via e-mail reviews, to a formal peer review meeting with the presence of quality assurance officers and writing of review documents. The over-the-shoulder variant is often very fruitful when you are stuck at some problem. Your colleague will often pinpoint the error and it is often trivial to fix. The pair programming technique makes full use of this effect. Two programmers work together at a workstation. The driver writes the code while the observer reviews each line as it is typed. Roles are switched frequently. However, for pushing your code to the main developing branch of your central repository, a formal tool-based peer review is recommended. Several tools for software peer review are available on the market. They will integrate with your software version control system and make the technical aspects of peer review simple. They also document the review process automatically.</p>
<h1><a class="anchor" id="CompileTimeTests"></a>
Compile-time tests</h1>
<blockquote class="doxtable">
<p>To err is human, to persist in error is diabolical. </p><div id="author"> Georges Canguilhem </div> </blockquote>
<p>What is better than to catch errors early, even before they are compiled. All assumptions made when designing and implementing the code which is testable at compile time, should be tested during compilation. C++11 and later has the declaration &lsquo;static_assert&rsquo;. </p><div class="fragment"><div class="line">static_assert(constant-expression,<span class="keywordtype">string</span>-literal)</div></div><!-- fragment --><p> This is a compile-time check. If the constant-expression evaluates to false, the compilation fails with the string-literal as an error message. The &lsquo;static_assert&rsquo; declaration is especially useful in combination with the classes from the type support library classes of the &lsquo;&lt;type_traits&gt;&rsquo; header. It then provides safeguards against bugs created by wrongful use of templates. It also helps to reveal critical differences when porting software to new platforms, provided all assumptions have been expressed in static assert declarations.</p>
<p>A very simple example of its use is shown in the <a href="byte__swap_8h_source.html">byte swap code</a>.</p>
<h1><a class="anchor" id="RunTimeTests"></a>
Run-time tests</h1>
<blockquote class="doxtable">
<p>Fundamentally you can’t do your own QA, it’s a question of seeing you own blind spots. </p><div id="author"> Ron Avitzur </div> </blockquote>
<p>In the design by contract methodology, the design is expressed as assertions embedded in the source. For example, restrictions on input parameters (preconditions) are checked run-time. If a precondition is violated, an exception is thrown. For a simple example on the <a href="Circle_8cpp_source.html">radius of a circle</a>. These kind of run-time tests guards against wrong use of classes and functions.</p>
<p>Similarly, one could put in postconditions on floating point computations to ensure that the result never is a NaN (Not a Number).</p>
<p>Of course, spreading extra run-time checks everywhere in your code will slow it down. If speed is important in your application, you could keep the tests on during development and testing phase and disable them with a preprocessor directive in the final release shipped to customers. In the examples of Dbc macros we have used the &lsquo;NDEBUG&rsquo; macro to turn off checking. If &lsquo;NDEBUG&rsquo; is set, the &lsquo;PRECONDITION&rsquo;, &lsquo;POSTCONDITION&rsquo; and &lsquo;INVARIANT&rsquo; macros all turn void. In real code you may want to have different testing levels depending on the state of your code, and possibly catch the exceptions and log the user input and the errors to a log file for easier debugging.</p>
<h1><a class="anchor" id="MemoryErrorTools"></a>
Memory error detection tools</h1>
<p>Another aspect of run-time testing is to run a memory error detector such as &lsquo;valgrind&rsquo;. Such tools detect use of uninitialized memory, errounous memory access, and memory leaks. Below is a transcript of a valgrind run of the example unit tests: </p><div class="fragment"></div><!-- fragment --><p> In this case there were no memory leaks and no errors. If your code produces strange errors, perhaps errors that are irreproducible, then always check the code with a memory error detector before debugging. It could save you a lot of time. For the same reason, all baselines of your main branch should be checked for memory errors.</p>
<h1><a class="anchor" id="Continuous"></a>
Continuous integration and automated testing</h1>
<p>Continuous integration (CI) <a class="el" href="citelist.html#CITEREF_Booch:CI">[2]</a> is a software development practice where members of a team regularly integrate their work into a common mainline. Each such integration is followed by automated builds and tests. This ensures that new defects are detected as quickly as possible. In addition, by integrating often, there are fewer merging problems since each developer's own branch never deviates far from the main branch.</p>
<p>The key point&mdash;from a bug trapping point of view&mdash;is that the test procedures should be triggered <em>automatically</em>. This ensures that the tests are done and bugs are fixed as early as possible. Jenkins <a class="el" href="citelist.html#CITEREF_Jenkins">[18]</a> is a popular tool for automatization of tests.</p>
<h1><a class="anchor" id="UnitTesting"></a>
Unit testing</h1>
<blockquote class="doxtable">
<p>Beware of the above code; I have only proved it correct, not tried it. </p><div id="author"> Donald Knuth </div> </blockquote>
<p>A software unit could be a component, a module, a class, or a method. In unit testing a unit refers to the smallest testable part of an application. In C++ a unit is a usually a class or invidual methods. Ideally, each test should be independent from the others, but in reality methods of a class often depend on other methods from the same or from other classes. Hence, any dependencies to other units should be eliminated by the use of fakes, stubs or mocks (a dear child has many names). Otherwise there will be dependencies to other units and the tests will be less reliable, and more costly to maintain. In order to be able to control the dependencies of a unit, the code has to be designed so that one is able to inject mocks for all dependencies when setting up the test. In the code example &lsquo;class NeedsEngine&rsquo; we give an example of a pattern <a class="el" href="citelist.html#CITEREF_BeautifulInjection">[20]</a>, for injecting the engine dependency in a car class.</p>
<p>Each method should be tested in a separate test so that the failure of a test easily is traced back to the offending unit. Unit tests are often collected in test suites, where a suite often correspond to a whole class or a module.</p>
<p>The unit tests determine whether a given feature works as intended by the programmer. For each class and function, the programmers write as many automated tests as they can think of that might catch <span class="quote">broken code</span>. Only when all tests, new and old, run successfully, the code is integrated into the development tree. In this way, a rich collection of test suites guard against introduction of new errors, or <em>regressions</em>, in existing functionality after adding new features or after refactoring. This comes with a bit of a caveat, however. Having many unit tests, increases the cost of changing code.</p>
<p>It is very important to document the tests and the expected return values (the <span class="quote">truth</span>) for all functions so that the tests can be reviewed and understood by others. It follows that the test code should follow the same stringent coding standard as the production code itself.</p>
<p>The test and its explanation document the <em>specification</em> of each tested unit. In test-driven development (TDD) requirements are translated into test cases and the software is developed so that it passes the new tests <a class="el" href="citelist.html#CITEREF_TDD:Wikipedia">[19]</a>. TDD is characterized by a short development cycle. This development process ensures that the code is designed to be testable.</p>
<p>It is, however, important to realize that unit testing is <em>not</em> an effective way of detecting bugs. More often than not, bugs arise in the interaction between software units. Even if components A and B both work independently, they may not be compatible with one another or configured correctly. Such errors are caught during <em>integration testing</em>.</p>
<h2><a class="anchor" id="GoogleTest"></a>
The Google Test framework</h2>
<p>Implementing tests for all classes and methods may appear to be a daunting task, and indeed it is a big task. As always in programming, the trick is to divide and conquer. If you write tests for each function you write, then you have a large test suite before you know it. The job of keeping track of the tests, and making sure that they are run, is a simple routine task which should be automated.</p>
<p>Several frameworks exist for unit testing of C++ code. Do an internet search for <span class="quote">Unit testing C++</span> and you should get a lot of hits. In this book the concept of unit testing is illustrated with GoogleTest, also known as gtest, <a class="el" href="citelist.html#CITEREF_GoogleTest">[9]</a>. This framework was chosen because it has: </p><ol>
<li>
Minimal coding overhead when writing tests. </li>
<li>
Automatic test registration. </li>
<li>
Support for suites or grouping of tests. </li>
<li>
Support for fixtures. </li>
<li>
Good scalability. </li>
<li>
Possibility to run subsets of the tests using wildcards. </li>
</ol>
<p>For examples of unit tests written with GoogleTest, see <a class="el" href="group__UnitTests.html">"Unit Tests"</a></p>
<h2><a class="anchor" id="GoodUnitTests"></a>
Writing good unit tests</h2>
<p>To be able to write good unit tests, the code should be testable (see the chapter <a class="el" href="page5.html">Testable code</a>). By testable I mean that the unit is designed so that tests can be written with a reasonable amount of effort. Moreover, changes to other units should have minimal impact on the tests.</p>
<p>A good unit test suite should cover all relevant input, forcing the code to visit all possible branches during the tests. Use tools for checking test coverage (see <a href="bugtrap/code/index.html">Test coverage</a>). Parameter space should be tested at the limits, beyond the legal limits, and at special values in between.</p>
<p>Keeping the classes small and focused helps to make the task of writing unit tests managable. Remember that as you increase the number of parameters to a function, there is a combinatorical explosion of combinations to test. Consider a n-parameter function \(f(x_0,\ldots, x_{n-1})\) with valid parameters \(x_i \in [x_{\text{min}(i)},x_{\text{max}(i)}]\). Testing the limits, beyond limits and in the middle, gives five test values per parameter. There are then \(5^n\) combinations to test for this function. Already at three parameters, the situation is daunting.</p>
<p>Run-time errors such as illegal input to functions should be caught by assertions or exceptions. For very well tested code, where performance is of the essence, and where run-time errors are guaranteed never to happen, the assertions can be turned off in release builds.</p>
<h1><a class="anchor" id="Integration"></a>
Integration  testing</h1>
<blockquote class="doxtable">
<p>The bitterness of poor quality remains long after the sweetness of meeting the schedule has been forgotten. </p><div id="author"> Anonymous </div> </blockquote>
<p>Once all the individual units have been created and unit tested, they are integrated into larger modules. The collective behaviour of this integrated whole needs to be tested, too. The main goal of integration testing is to test the interaction and cooperation between the units. Does the whole work as intended in all situations? Many bugs are created in the integration phase, because the developer assembling the parts does not know the correct use of the parts. This happens if the interfaces are poorly documented, or the documentation is not read because it is hard to find or there is a time pressure because of a rapidly approaching delivery date.</p>
<h1><a class="anchor" id="System"></a>
System  testing</h1>
<blockquote class="doxtable">
<p>The problem is not that testing is the bottleneck. The problem is that you don’t know what’s in the bottle. That’s a problem that testing addresses. </p><div id="author"> Michael Bolton </div> </blockquote>
<p>System testing is done on the whole system. It could either be done after all the other parts have been integrated and assembled into a complete system (bottom-up testing), or at an early stage where subsystems have been replaced with stubs or mockups (top-down testing). More often than not, a combination of these two scheemes is used. Postponing system testing until everything else is finished is very risky. If something is amiss, a lot of interfaces may have to change. On the other hand, a full verification of all the system requirements is not possible until all the code is in place. Hence, a combined approach is called for.</p>
<p>Ideally there should be one test for each system requirement. A test description for each of these tests should be available when implementation of its realization starts. Then one can implement a test before the requirement is met by the system.</p>
<p>Normally there are performance requirements on the system. Hence, there should also be performance tests in the test suite. Obviously the performance tests should be built with the relevant compilator optimization flags turned on.</p>
<p>The collection of system tests should be maintained and refined during the development stages, and after each code commit, there should be a system for automatically running all the unit tests and system tests automatically both in debug mode and in release mode (see e.g. <a class="el" href="citelist.html#CITEREF_Jenkins">[18]</a>). The tests should be built and run on all the relevant computer platforms.</p>
<h1><a class="anchor" id="BugTrapConclusion"></a>
Concluding remarks on bug traps</h1>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- HTML footer for doxygen 1.8.14-->
<!-- start footer part -->
<div class="center">
<div class="pagination">
  <a href="index.html">Preface</a>
  <a href="page1.html">Punctuation</a>
  <a href="page2.html">Origins</a>
  <a href="page3.html">Documentation</a>
  <a href="page4.html">Bug traps</a>
  <a href="page5.html">Testable</a>
  <a href="page6.html">Process</a>
  <a href="page7.html">Conclusion</a>
  <a href="citelist.html">Bibliography</a>
</div>
</div>
</body>
</html>
